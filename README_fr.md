# Python A2A

<div align="center">

[![PyPI version](https://img.shields.io/pypi/v/python-a2a.svg)](https://pypi.org/project/python-a2a/)
[![Python Versions](https://img.shields.io/pypi/pyversions/python-a2a.svg)](https://pypi.org/project/python-a2a/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI Downloads](https://static.pepy.tech/badge/python-a2a)](https://pepy.tech/project/python-a2a)
[![Documentation Status](https://readthedocs.org/projects/python-a2a/badge/?version=latest)](https://python-a2a.readthedocs.io/en/latest/?badge=latest)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
[![UV Compatible](https://img.shields.io/badge/UV-Compatible-5C63FF.svg)](https://github.com/astral-sh/uv)
[![GitHub stars](https://img.shields.io/github/stars/themanojdesai/python-a2a?style=social)](https://github.com/themanojdesai/python-a2a/stargazers)

  <p>
      <a href="README.md">English</a> | <a href="README_zh.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> | <a href="README_ja.md">Êó•Êú¨Ë™û</a> | <a href="README_es.md">Espa√±ol</a> | <a href="README_de.md">Deutsch</a> | <a href="README_fr.md">Fran√ßais</a>
      <!-- Add other languages here like: | <a href="README_de.md">Deutsch</a> -->
  </p>

**Impl√©mentation officielle Python du protocole Google Agent-to-Agent (A2A), avec int√©gration du Model Context Protocol (MCP)**

</div>

## üåü Aper√ßu

Python A2A est une biblioth√®que compl√®te et pr√™te pour la production pour impl√©menter le [protocole Agent-to-Agent (A2A) de Google](https://google.github.io/A2A/), avec un support complet pour le [Model Context Protocol (MCP)](https://contextual.ai/introducing-mcp/). Elle fournit toutes les fonctionnalit√©s n√©cessaires pour construire un √©cosyst√®me d'agents d'IA interoperables capables de collaborer en douceur pour r√©soudre des probl√®mes complexes.

Le protocole A2A √©tablit un format de communication standard permettant aux agents d'IA d'interagir, tandis que le MCP √©tend cette capacit√© en fournissant une m√©thode standardis√©e pour que les agents acc√®dent √† des outils et des sources de donn√©es externes. Python A2A rend ces protocoles faciles √† utiliser via une API intuitive, permettant aux d√©veloppeurs de construire des syst√®mes d'agents d'IA complexes.

## üìã Nouveaut√©s de la version v0.5.X

- **D√©couverte d'agents**: Support int√©gr√© pour le registre et la d√©couverte d'agents avec une compatibilit√© totale avec le protocole A2A de Google
- **Int√©gration LangChain**: Int√©gration fluide avec les outils et agents de LangChain
- **√âcosyst√®me d'outils √©tendu**: Utilisez des outils provenant √† la fois de LangChain et de MCP dans n'importe quel agent
- **Interop√©rabilit√© accrue des agents**: Conversion entre agents A2A et agents LangChain
- **Moteur de workflow hybride**: Cr√©ation de workflows combinant les deux √©cosyst√®mes
- **D√©veloppement simplifi√© des agents**: Acc√®s imm√©diat √† des milliers d'outils pr√©d√©finis
- **Architecture de streaming avanc√©e**: Streaming am√©lior√© avec les √©v√©nements envoy√©s par le serveur (SSE), un meilleur gestion des erreurs et des m√©canismes de secours robustes
- **Streaming bas√© sur les t√¢ches**: Nouvelle m√©thode `tasks_send_subscribe` pour le streaming des mises √† jour de t√¢ches en temps r√©el
- **API de gestion des blocs de streaming**: Am√©lioration de la gestion des blocs avec la classe `StreamingChunk` pour des donn√©es structur√©es
- **Support multi-point de terminaison**: D√©couverte automatique et m√©canismes de secours entre plusieurs points de terminaison de streaming

## üìã Nouveaut√©s de la version v0.4.X

- **Syst√®me de r√©seau d'agents**: Gestion et d√©couverte de multiples agents avec la nouvelle classe `AgentNetwork`
- **Streaming en temps r√©el**: Impl√©mentation de r√©ponses en streaming avec `StreamingClient` pour des interfaces utilisateur r√©actives
- **Moteur de workflow**: D√©finition de workflows complexes de multiples agents √† l'aide de l'API fluide avec des branchements conditionnels et une ex√©cution parall√®le
- **Routeur d'IA**: Routage automatique des requ√™tes vers l'agent le plus appropri√© avec `AIAgentRouter`
- **Interface en ligne de commande**: Contr√¥le des agents depuis le terminal avec l'outil CLI
- **Support asynchrone am√©lior√©**: Meilleure prise en charge de async/await √† travers toute la biblioth√®que
- **Nouvelles options de connexion**: Gestion am√©lior√©e des erreurs et logique de r√©essai pour une communication d'agents plus robuste

## ‚ú® Pourquoi choisir Python A2A ?

- **Impl√©mentation compl√®te**: Impl√©mente int√©gralement la sp√©cification A2A officielle sans compromis
- **D√©couverte d'agents**: Registre et d√©couverte d'agents int√©gr√©s pour la cr√©ation d'√©cosyst√®mes d'agents
- **Int√©gration MCP**: Support de premier plan pour le Model Context Protocol pour des agents utilisant des outils puissants
- **Pr√™t pour l'entreprise**: Con√ßu pour les environnements de production avec une gestion robuste des erreurs et une validation rigoureuse
- **Ind√©pendant du framework**: Fonctionne avec n'importe quel framework Python (Flask, FastAPI, Django, etc.)
- **Flexibilit√© du fournisseur LLM**: Int√©grations natives avec OpenAI, Anthropic, AWS Bedrock, et plus encore
- **D√©pendances minimales**: La fonctionnalit√© de base n√©cessite uniquement la biblioth√®que `requests`
- **Exp√©rience d√©veloppeur exceptionnelle**: Documentation compl√®te, indices de type et exemples

## üì¶ Installation

### Avec pip (m√©thode traditionnelle)

Installez le paquet de base avec toutes les d√©pendances:

```bash
pip install python-a2a  # Inclut LangChain, MCP et d'autres int√©grations
```

Ou installez des composants sp√©cifiques selon vos besoins:

```bash
# Pour le support du serveur bas√© sur Flask
pip install "python-a2a[server]"

# Pour l'int√©gration OpenAI
pip install "python-a2a[openai]"

# Pour l'int√©gration Anthropic Claude
pip install "python-a2a[anthropic]"

# Pour l'int√©gration AWS-Bedrock
pip install "python-a2a[bedrock]"

# Pour le support MCP (Model Context Protocol)
pip install "python-a2a[mcp]"

# Pour toutes les d√©pendances optionnelles
pip install "python-a2a[all]"
```

### Avec UV (recommand√©)

[UV](https://github.com/astral-sh/uv) est un outil moderne de gestion de paquets Python plus rapide et plus fiable que pip. Pour installer avec UV:

```bash
# Installez UV si ce n'est pas d√©j√† fait
curl -LsSf https://astral.sh/uv/install.sh | sh

# Installez le paquet de base
uv add python-a2a
```

### Installation pour le d√©veloppement

Pour le d√©veloppement, UV est recommand√© pour sa vitesse:

```bash
# Clonez le d√©p√¥t
git clone https://github.com/themanojdesai/python-a2a.git
cd python-a2a

# Cr√©ez un environnement virtuel et installez les d√©pendances de d√©veloppement
uv venv
source .venv/bin/activate  # Sous Windows: .venv\Scripts\activate
uv pip install -e ".[dev]"
```

> üí° **Astuce**: Cliquez sur les blocs de code pour les copier dans votre presse-papiers.

## üöÄ Exemples d'utilisation rapides

### 1. Cr√©er un agent A2A simple avec des comp√©tences

```python
from python_a2a import A2AServer, skill, agent, run_server, TaskStatus, TaskState

@agent(
    name="Weather Agent",
    description="Provides weather information",
    version="1.0.0"
)
class WeatherAgent(A2AServer):
    
    @skill(
        name="Get Weather",
        description="Get current weather for a location",
        tags=["weather", "forecast"]
    )
    def get_weather(self, location):
        """Get weather for a location."""
        # Impl√©mentation fictive
        return f"It's sunny and 75¬∞F in {location}"
    
    def handle_task(self, task):
        # Extraire l'emplacement √† partir du message
        message_data = task.message or {}
        content = message_data.get("content", {})
        text = content.get("text", "") if isinstance(content, dict) else ""
        
        if "weather" in text.lower() and "in" in text.lower():
            location = text.split("in", 1)[1].strip().rstrip("?.")
            
            # Obtenir la m√©t√©o et cr√©er une r√©ponse
            weather_text = self.get_weather(location)
            task.artifacts = [{
                "parts": [{"type": "text", "text": weather_text}]
            }]
            task.status = TaskStatus(state=TaskState.COMPLETED)
        else:
            task.status = TaskStatus(
                state=TaskState.INPUT_REQUIRED,
                message={"role": "agent", "content": {"type": "text", 
                         "text": "Please ask about weather in a specific location."}}
            )
        return task

# D√©marrer le serveur
if __name__ == "__main__":
    agent = WeatherAgent()
    run_server(agent, port=5000)
```

### 2. Construire un r√©seau d'agents avec plusieurs agents

```python
from python_a2a import AgentNetwork, A2AClient, AIAgentRouter

# Cr√©er un r√©seau d'agents
network = AgentNetwork(name="Travel Assistant Network")

# Ajouter des agents au r√©seau
network.add("weather", "http://localhost:5001")
network.add("hotels", "http://localhost:5002")
network.add("attractions", "http://localhost:5003")

# Cr√©er un routeur pour diriger intelligemment les requ√™tes vers l'agent le plus appropri√©
router = AIAgentRouter(
    llm_client=A2AClient("http://localhost:5000/openai"),  # LLM pour les d√©cisions de routage
    agent_network=network
)

# Router une requ√™te vers l'agent appropri√©
agent_name, confidence = router.route_query("What's the weather like in Paris?")
print(f"Routage vers {agent_name} avec {confidence:.2f} de confiance")

# Obtenir l'agent s√©lectionn√© et poser la question
agent = network.get_agent(agent_name)
response = agent.ask("What's the weather like in Paris?")
print(f"R√©ponse: {response}")

# Lister tous les agents disponibles
print("\nAgents disponibles:")
for agent_info in network.list_agents():
    print(f"- {agent_info['name']}: {agent_info['description']}")
```

### Streaming en temps r√©el

Obtenir des r√©ponses en temps r√©el des agents avec un support de streaming complet:

```python
import asyncio
from python_a2a import StreamingClient, Message, TextContent, MessageRole

async def main():
    client = StreamingClient("http://localhost:5000")
    
    # Cr√©er un message avec le param√®tre role requis
    message = Message(
        content=TextContent(text="Tell me about A2A streaming"),
        role=MessageRole.USER
    )
    
    # Streamer la r√©ponse et traiter les blocs en temps r√©el
    try:
        async for chunk in client.stream_response(message):
            # G√©rer diff√©rents formats de blocs (cha√Æne ou dictionnaire)
            if isinstance(chunk, dict):
                if "content" in chunk:
                    print(chunk["content"], end="", flush=True)
                elif "text" in chunk:
                    print(chunk["text"], end="", flush=True)
                else:
                    print(str(chunk), end="", flush=True)
            else:
                print(str(chunk), end="", flush=True)
    except Exception as e:
        print(f"Erreur de streaming: {e}")
```

Consultez le r√©pertoire `examples/streaming/` pour des exemples complets de streaming:

- **basic_streaming.py**: Impl√©mentation minimale de streaming (commencez ici!)
- **01_basic_streaming.py**: Introduction compl√®te aux bases du streaming
- **02_advanced_streaming.py**: Streaming avanc√© avec diff√©rentes strat√©gies de blocage
- **03_streaming_llm_integration.py**: Int√©gration du streaming avec les fournisseurs LLM
- **04_task_based_streaming.py**: Streaming bas√© sur les t√¢ches avec suivi de progression
- **05_streaming_ui_integration.py**: Int√©gration du streaming avec les interfaces utilisateur (CLI et web)
- **06_distributed_streaming.py**: Architecture de streaming distribu√©

### 3. Moteur de workflow

Le nouveau moteur de workflow permet de d√©finir des interactions complexes entre agents:

```python
from python_a2a import AgentNetwork, Flow
import asyncio

async def main():
    # Configurer le r√©seau d'agents
    network = AgentNetwork()
    network.add("research", "http://localhost:5001")
    network.add("summarizer", "http://localhost:5002")
    network.add("factchecker", "http://localhost:5003")
    
    # D√©finir un workflow pour la g√©n√©ration de rapports de recherche
    flow = Flow(agent_network=network, name="Research Report Workflow")
    
    # D'abord, collecter la recherche initiale
    flow.ask("research", "Research the latest developments in {topic}")
    
    # Ensuite, traiter les r√©sultats en parall√®le
    parallel_results = (flow.parallel()
        # Branche 1: Cr√©er un r√©sum√©
        .ask("summarizer", "Summarize this research: {latest_result}")
        # Branche 2: V√©rifier les faits cl√©s
        .branch()
        .ask("factchecker", "Verify these key facts: {latest_result}")
        # Fin du traitement parall√®le et collecte des r√©sultats
        .end_parallel(max_concurrency=2))
    
    # Extraire des insights bas√©s sur les r√©sultats de v√©rification
    flow.execute_function(
        lambda results, context: f"Summary: {results['1']}\nVerified Facts: {results['2']}",
        parallel_results
    )
    
    # Ex√©cuter le workflow
    result = await flow.run({
        "topic": "quantum computing advancements in the last year"
    })
    
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### 4. Routeur d'IA

Routage intelligent pour s√©lectionner l'agent le plus appropri√© pour chaque requ√™te:

```python
from python_a2a import AgentNetwork, AIAgentRouter, A2AClient
import asyncio

async def main():
    # Cr√©er un r√©seau avec des agents sp√©cialis√©s
    network = AgentNetwork()
    network.add("math", "http://localhost:5001")
    network.add("history", "http://localhost:5002")
    network.add("science", "http://localhost:5003")
    network.add("literature", "http://localhost:5004")
    
    # Cr√©er un routeur utilisant un LLM pour la prise de d√©cision
    router = AIAgentRouter(
        llm_client=A2AClient("http://localhost:5000/openai"),
        agent_network=network
    )
    
    # Exemples de requ√™tes √† router
    queries = [
        "What is the formula for the area of a circle?",
        "Who wrote The Great Gatsby?",
        "When did World War II end?",
        "How does photosynthesis work?",
        "What are Newton's laws of motion?"
    ]
    
    # Router chaque requ√™te vers l'agent le plus appropri√©
    for query in queries:
        agent_name, confidence = router.route_query(query)
        agent = network.get_agent(agent_name)
        
        print(f"Requ√™te: {query}")
        print(f"Rout√©e vers: {agent_name} (confiance: {confidence:.2f})")
        
        # Obtenir la r√©ponse de l'agent s√©lectionn√©
        response = agent.ask(query)
        print(f"R√©ponse: {response[:100]}...\n")

if __name__ == "__main__":
    asyncio.run(main())
```

### 5. D√©finir des workflows complexes avec plusieurs agents

```python
from python_a2a import AgentNetwork, Flow, AIAgentRouter
import asyncio

async def main():
    # Cr√©er un r√©seau d'agents
    network = AgentNetwork()
    network.add("weather", "http://localhost:5001")
    network.add("recommendations", "http://localhost:5002")
    network.add("booking", "http://localhost:5003")
    
    # Cr√©er un routeur
    router = AIAgentRouter(
        llm_client=network.get_agent("weather"),  # Utiliser un agent comme LLM pour le routage
        agent_network=network
    )
    
    # D√©finir un workflow avec de la logique conditionnelle
    flow = Flow(agent_network=network, router=router, name="Travel Planning Workflow")
    
    # Commencer par obtenir la m√©t√©o
    flow.ask("weather", "What's the weather in {destination}?")
    
    # Branche conditionnelle bas√©e sur la m√©t√©o
    flow.if_contains("sunny")
    
    # Si ensoleill√©, recommander des activit√©s en plein air
    flow.ask("recommendations", "Recommend outdoor activities in {destination}")
    
    # Fin de la condition et ajout d'une branche else
    flow.else_branch()
    
    # Si non ensoleill√©, recommander des activit√©s en int√©rieur
    flow.ask("recommendations", "Recommend indoor activities in {destination}")
    
    # Fin du bloc if-else
    flow.end_if()
    
    # Ajouter des √©tapes de traitement parall√®le
    (flow.parallel()
        .ask("booking", "Find hotels in {destination}")
        .branch()
        .ask("booking", "Find restaurants in {destination}")
        .end_parallel())
    
    # Ex√©cuter le workflow avec un contexte initial
    result = await flow.run({
        "destination": "Paris",
        "travel_dates": "June 12-20"
    })
    
    print("R√©sultat du workflow:")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### 6. Utiliser l'interface en ligne de commande

```bash
# Envoyer un message √† un agent
a2a send http://localhost:5000 "What is artificial intelligence?"

# Streamer une r√©ponse en temps r√©el
a2a stream http://localhost:5000 "Generate a step-by-step tutorial for making pasta"

# D√©marrer un serveur A2A aliment√© par OpenAI
a2a openai --model gpt-4 --system-prompt "You are a helpful coding assistant"

# D√©marrer un serveur A2A aliment√© par Anthropic
a2a anthropic --model claude-3-opus-20240229 --system-prompt "You are a friendly AI teacher"

# D√©marrer un serveur MCP avec des outils
a2a mcp-serve --name "Data Analysis MCP" --port 5001 --script analysis_tools.py

# D√©marrer un agent A2A avec MCP
a2a mcp-agent --servers data=http://localhost:5001 calc=http://localhost:5002

# Appeler un outil MCP directement
a2a mcp-call http://localhost:5001 analyze_csv --params file=data.csv columns=price,date

# G√©rer les r√©seaux d'agents
a2a network --add weather=http://localhost:5001 travel=http://localhost:5002 --save network.json

# Ex√©cuter un workflow √† partir d'un script
a2a workflow --script research_workflow.py --context initial_data.json
```

## üîÑ Int√©gration LangChain (Nouveau dans v0.5.X)

Python A2A inclut une int√©gration LangChain int√©gr√©e, facilitant la combinaison des meilleures fonctionnalit√©s des deux √©cosyst√®mes:

### 1. Conversion des outils MCP vers LangChain

```python
from python_a2a.mcp import FastMCP, text_response
from python_a2a.langchain import to_langchain_tool

# Cr√©er un serveur MCP avec un outil
mcp_server = FastMCP(name="Basic Tools", description="Simple utility tools")

@mcp_server.tool(
    name="calculator",
    description="Calculate a mathematical expression"
)
def calculator(input):
    """Simple calculator that evaluates an expression."""
    try:
        result = eval(input)
        return text_response(f"Result: {result}")
    except Exception as e:
        return text_response(f"Error: {e}")

# D√©marrer le serveur
import threading, time
def run_server(server, port):
    server.run(host="0.0.0.0", port=port)
server_thread = threading.Thread(target=run_server, args=(mcp_server, 5000), daemon=True)
server_thread.start()
time.sleep(2)  # Permettre au serveur de d√©marrer

# Convertir l'outil MCP vers LangChain
calculator_tool = to_langchain_tool("http://localhost:5000", "calculator")

# Utiliser l'outil dans LangChain
result = calculator_tool.run("5 * 9 + 3")
print(f"R√©sultat: {result}")
```

### 2. Conversion des outils LangChain vers un serveur MCP

```python
from langchain.tools import Tool
from langchain_core.tools import BaseTool
from python_a2a.langchain import to_mcp_server

# Cr√©er des outils LangChain
def calculator(expression: str) -> str:
    """Evaluate a mathematical expression"""
    try:
        result = eval(expression)
        return f"Result: {expression} = {result}"
    except Exception as e:
        return f"Error: {e}"

calculator_tool = Tool(
    name="calculator",
    description="Evaluate a mathematical expression",
    func=calculator
)

# Convertir vers un serveur MCP
mcp_server = to_mcp_server(calculator_tool)

# D√©marrer le serveur
mcp_server.run(port=5000)
```

### 3. Conversion des composants LangChain vers des serveurs A2A

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from python_a2a import A2AClient, run_server
from python_a2a.langchain import to_a2a_server

# Cr√©er un LLM LangChain
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# Convertir le LLM vers un serveur A2A
llm_server = to_a2a_server(llm)

# Cr√©er une cha√Æne simple
template = "You are a helpful travel guide.\n\nQuestion: {query}\n\nAnswer:"
prompt = PromptTemplate.from_template(template)
travel_chain = prompt | llm | StrOutputParser()

# Convertir la cha√Æne vers un serveur A2A
travel_server = to_a2a_server(travel_chain)

# D√©marrer les serveurs dans des threads d'arri√®re-plan
import threading
llm_thread = threading.Thread(
    target=lambda: run_server(llm_server, port=5001),
    daemon=True
)
llm_thread.start()

travel_thread = threading.Thread(
    target=lambda: run_server(travel_server, port=5002),
    daemon=True
)
travel_thread.start()

# Tester les serveurs
llm_client = A2AClient("http://localhost:5001")
travel_client = A2AClient("http://localhost:5002")

llm_result = llm_client.ask("What is the capital of France?")
travel_result = travel_client.ask('{"query": "What are some must-see attractions in Paris?"}')
```

### 4. Conversion des agents A2A vers des agents LangChain

```python
from python_a2a.langchain import to_langchain_agent

# Convertir un agent A2A vers un agent LangChain
langchain_agent = to_langchain_agent("http://localhost:5000")

# Utiliser l'agent dans LangChain
result = langchain_agent.invoke("What are some famous landmarks in Paris?")
print(result.get('output', ''))

# Utiliser dans un pipeline LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(temperature=0)
prompt = ChatPromptTemplate.from_template(
    "Generate a specific, detailed travel question about {destination}."
)

# Cr√©er un pipeline avec l'agent converti
chain = (
    prompt |
    llm |
    StrOutputParser() |
    langchain_agent |
    (lambda x: f"Travel Info: {x.get('output', '')}")
)

result = chain.invoke({"destination": "Japan"})
print(result)
```

LangChain est automatiquement install√© en tant que d√©pendance avec python-a2a, donc tout fonctionne imm√©diatement:

```bash
pip install python-a2a
# C'est tout! LangChain est inclus automatiquement
```

## üß© Fonctionnalit√©s cl√©s

### R√©seaux d'agents

Python A2A inclut maintenant un syst√®me puissant pour g√©rer plusieurs agents:

```python
from python_a2a import AgentNetwork, A2AClient

# Cr√©er un r√©seau d'agents
network = AgentNetwork(name="Medical Assistant Network")

# Ajouter des agents de diff√©rentes mani√®res
network.add("diagnosis", "http://localhost:5001")  # √Ä partir d'une URL
network.add("medications", A2AClient("http://localhost:5002"))  # √Ä partir d'une instance client

# D√©couvrir des agents √† partir d'une liste d'URLs
discovered_count = network.discover_agents([
    "http://localhost:5003",
    "http://localhost:5004",
    "http://localhost:5005"
])
print(f"{discovered_count} nouveaux agents d√©couverts")

# Lister tous les agents dans le r√©seau
for agent_info in network.list_agents():
    print(f"Agent: {agent_info['name']}")
    print(f"URL: {agent_info['url']}")
    if 'description' in agent_info:
        print(f"Description: {agent_info['description']}")
    print()

# Obtenir un agent sp√©cifique
agent = network.get_agent("diagnosis")
response = agent.ask("What are the symptoms of the flu?")
```

### 7. D√©couverte et registre d'agents

```python
from python_a2a import AgentCard, A2AServer, run_server
from python_a2a.discovery import AgentRegistry, run_registry, enable_discovery, DiscoveryClient
import threading
import time

# Cr√©er un serveur de registre
registry = AgentRegistry(
    name="A2A Registry Server",
    description="Registre central pour la d√©couverte d'agents"
)

# D√©marrer le registre dans un thread d'arri√®re-plan
registry_port = 8000
thread = threading.Thread(
    target=lambda: run_registry(registry, host="0.0.0.0", port=registry_port),
    daemon=True
)
thread.start()
time.sleep(1)  # Permettre au registre de d√©marrer

# Cr√©er un agent d'exemple
agent_card = AgentCard(
    name="Weather Agent",
    description="Provides weather information",
    url="http://localhost:8001",
    version="1.0.0",
    capabilities={
        "weather_forecasting": True,
        "google_a2a_compatible": True  # Activer la compatibilit√© A2A de Google
    }
)
agent = A2AServer(agent_card=agent_card)

# Activer la d√©couverte - cela enregistre l'agent dans le registre
registry_url = f"http://localhost:{registry_port}"
discovery_client = enable_discovery(agent, registry_url=registry_url)

# D√©marrer l'agent dans un thread s√©par√©
agent_thread = threading.Thread(
    target=lambda: run_server(agent, host="0.0.0.0", port=8001),
    daemon=True
)
agent_thread.start()
time.sleep(1)  # Permettre √† l'agent de d√©marrer

# Cr√©er un client de d√©couverte pour trouver des agents
client = DiscoveryClient(agent_card=None)  # Aucune carte d'agent n√©cessaire pour la d√©couverte seule
client.add_registry(registry_url)

# D√©couvrir tous les agents
agents = client.discover()
print(f"{len(agents)} agents d√©couverts:")
for agent in agents:
    print(f"- {agent.name} √† {agent.url}")
    print(f"  Capacit√©s: {agent.capabilities}")
```

## üìñ Architecture et principes de conception

Python A2A est construit sur trois principes de conception cl√©s:

1. **Protocole en premier**: Respect strict des sp√©cifications des protocoles A2A et MCP pour une interop√©rabilit√© maximale

2. **Modularit√©**: Tous les composants sont con√ßus pour √™tre composable et rempla√ßable

3. **Am√©lioration progressive**: Commencer simple et ajouter de la complexit√© uniquement si n√©cessaire

L'architecture se compose de huit composants principaux:

- **Mod√®les**: Structures de donn√©es repr√©sentant les messages A2A, les t√¢ches et les cartes d'agents
- **Client**: Composants pour envoyer des messages aux agents A2A et g√©rer les r√©seaux d'agents
- **Serveur**: Composants pour construire des agents compatibles A2A
- **D√©couverte**: M√©canismes de registre et de d√©couverte pour les √©cosyst√®mes d'agents
- **MCP**: Outils pour impl√©menter des serveurs et clients du Model Context Protocol
- **LangChain**: Composants pont pour l'int√©gration LangChain
- **Workflow**: Moteur pour orchestrer des interactions complexes entre agents
- **Utils**: Fonctions d'aide pour les t√¢ches courantes
- **CLI**: Interface en ligne de commande pour interagir avec les agents

## üó∫Ô∏è Cas d'utilisation

Python A2A peut √™tre utilis√© pour construire une vaste gamme de syst√®mes d'IA:

### Recherche et d√©veloppement

- **Cadre d'exp√©rimentation**: √âchange facile entre diff√©rents backends LLM tout en maintenant la m√™me interface d'agent
- **Benchmarks**: Comparer les performances des diff√©rentes impl√©mentations d'agents sur des t√¢ches standardis√©es
- **Assistants de recherche en streaming**: Cr√©er des outils de recherche r√©actifs avec une sortie en temps r√©el via le streaming

### Syst√®mes d'entreprise

- **Orchestration d'IA**: Coordonner plusieurs agents d'IA entre diff√©rents d√©partements via des r√©seaux d'agents
- **Int√©gration de syst√®mes h√©rit√©s**: Emballer les syst√®mes h√©rit√©s avec des interfaces A2A pour une accessibilit√© √† l'IA
- **Workflows complexes**: Cr√©er des processus m√©tier sophistiqu√©s avec des workflows multi-agents et des branchements conditionnels

### Applications orient√©es client

- **Assistants multi-√©tapes**: D√©couper les requ√™tes complexes des utilisateurs en sous-t√¢ches g√©r√©es par des agents sp√©cialis√©s
- **Agents utilisant des outils**: Connecter des LLM √† des agents de base de donn√©es, des agents de calcul, etc. via le MCP
- **Interfaces de chat en temps r√©el**: Construire des applications de chat r√©actives avec un support de r√©ponse en streaming

### √âducation et formation

- **√âducation en IA**: Cr√©er des syst√®mes √©ducatifs d√©montrant la collaboration entre agents
- **Environnements de simulation**: Construire des environnements simul√©s o√π plusieurs agents interagissent
- **Workflows √©ducatifs**: Concevoir des processus d'apprentissage √©tape par √©tape avec des boucles de feedback

## üõ†Ô∏è Exemples concrets

Consultez le r√©pertoire [`examples/`](https://github.com/themanojdesai/python-a2a/tree/main/examples) pour des exemples concrets, y compris:

- Syst√®mes de support client multi-agents
- Assistants de recherche aliment√©s par LLM avec acc√®s aux outils
- Impl√©mentations de streaming en temps r√©el
- Exemples d'int√©gration LangChain
- Impl√©mentations de serveurs MCP pour divers outils
- Exemples d'orchestration de workflows
- Gestion de r√©seaux d'agents

## üîÑ Projets li√©s

Voici quelques projets li√©s dans l'espace des agents d'IA et de l'interop√©rabilit√©:

- [**Google A2A**](https://github.com/google/A2A) - La sp√©cification officielle du protocole A2A de Google
- [**LangChain**](https://github.com/langchain-ai/langchain) - Framework pour construire des applications avec des LLM
- [**AutoGen**](https://github.com/microsoft/autogen) - Framework de Microsoft pour les conversations multi-agents
- [**CrewAI**](https://github.com/joaomdmoura/crewAI) - Framework pour orchestrer des agents de r√¥le
- [**MCP**](https://github.com/contextco/mcp) - Le Model Context Protocol pour les agents utilisant des outils

## üë• Contributeurs

Merci √† tous nos contributeurs!

<a href="https://github.com/themanojdesai/python-a2a/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=themanojdesai/python-a2a" />
</a>

Souhaitez-vous contribuer? Consultez notre [guide de contribution](https://python-a2a.readthedocs.io/en/latest/contributing.html).

## ü§ù Communaut√© et support

- **[GitHub Issues](https://github.com/themanojdesai/python-a2a/issues)**: Signaler des bugs ou demander des fonctionnalit√©s
- **[GitHub Discussions](https://github.com/themanojdesai/python-a2a/discussions)**: Poser des questions et partager des id√©es
- **[Guide de contribution](https://python-a2a.readthedocs.io/en/latest/contributing.html)**: Apprendre comment contribuer au projet
- **[ReadTheDocs](https://python-a2a.readthedocs.io/en/latest/)**: Visitez notre site de documentation

## üìù Citer ce projet

Si vous utilisez Python A2A dans vos travaux de recherche ou acad√©miques, veuillez le citer comme suit:

```
@software{desai2025pythona2a,
  author = {Desai, Manoj},
  title = {Python A2A: A Comprehensive Implementation of the Agent-to-Agent Protocol},
  url = {https://github.com/themanojdesai/python-a2a},
  version = {0.5.0},
  year = {2025},
}
```

## ‚≠ê √âtoilez ce d√©p√¥t

Si vous trouvez cette biblioth√®que utile, n'h√©sitez pas √† lui donner une √©toile sur GitHub! Cela aide les autres √† la d√©couvrir et motive le d√©veloppement ult√©rieur.

[![GitHub Repo stars](https://img.shields.io/github/stars/themanojdesai/python-a2a?style=social)](https://github.com/themanojdesai/python-a2a/stargazers)

### Historique des √©toiles

[![Star History Chart](https://api.star-history.com/svg?repos=themanojdesai/python-a2a&type=Date)](https://star-history.com/#themanojdesai/python-a2a&Date)

## üôè Remerciements

- √Ä l'√©quipe [Google A2A](https://github.com/google/A2A) pour avoir cr√©√© le protocole A2A
- √Ä l'√©quipe [Contextual AI](https://contextual.ai/) pour le Model Context Protocol
- √Ä l'√©quipe [LangChain](https://github.com/langchain-ai) pour leur framework puissant de LLM
- √Ä tous nos [contributeurs](https://github.com/themanojdesai/python-a2a/graphs/contributors) pour leurs pr√©cieux commentaires

## üë®‚Äçüíª Auteur

**Manoj Desai**

- GitHub: [themanojdesai](https://github.com/themanojdesai)
- LinkedIn: [themanojdesai](https://www.linkedin.com/in/themanojdesai/)
- Medium: [@the_manoj_desai](https://medium.com/@the_manoj_desai)

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

---

Cr√©√© avec ‚ù§Ô∏è par [Manoj Desai](https://github.com/themanojdesai)
